{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Baseline_2_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DbMUk6vM3Z83"},"source":["# Customized baseline model with LSTM price prediction"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jbhC0vnO3Z84"},"source":["#### Table of contents\n","- LSTM price change prediction\n","- Conic optimizer to transform price change data to optimal portfolio weights"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2kYQEwor3Z86"},"source":["# 1. LSTM Price Change Prediction"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":2785,"status":"ok","timestamp":1575743483568,"user":{"displayName":"Ziyan Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDy4jjH-H5uFROlqAKrvnbKMknaSFS1hPNcCzqm4Q=s64","userId":"17713098623346557830"},"user_tz":300},"id":"N59hskR-3mOV","outputId":"ed88d656-cd0e-4070-f054-9aa096cd5643","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["\"\"\" mount drive \"\"\"\n","\n","import os\n","\n","GOOGLE_DIR = '/content/drive/My Drive/DL_Project/Kaggle_New'\n","LOCAL_DIR = '/home/ziyan/Desktop/Deep Learning Project/Kaggle_New'\n","\n","def colab_mount_google_drive():\n","    drive.mount('/content/drive', force_remount=True)\n","    os.chdir(GOOGLE_DIR)\n","    os.listdir()\n","\n","def mount_local_drive():\n","    os.chdir(LOCAL_DIR)\n","    os.listdir()\n","\n","try:\n","    from google.colab import drive\n","    colab_mount_google_drive()\n","    DIR = GOOGLE_DIR\n","    print('Mounted google drive')\n","except ModuleNotFoundError:\n","    mount_local_drive\n","    DIR = LOCAL_DIR\n","    print('Mounted local drive')\n","  \n","print(os.listdir())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted local drive\n","['Baseline_2_LSTM.ipynb', 'cudlfinance.zip', 'Baseline_2_LSTM (3).ipynb', 'sample_submission.csv', 'Baseline_2_LSTM (2).ipynb', 'Baseline_2_LSTM (1).ipynb', 'prediction', 'data', 'train_updated.csv', '.ipynb_checkpoints']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0CxTSWG-3Z87"},"source":["## Preprocess data\n","We convert price data to log natural price change data by performing:\n","ln(P[t] / P[t-1])"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":2763,"status":"ok","timestamp":1575743483569,"user":{"displayName":"Ziyan Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDy4jjH-H5uFROlqAKrvnbKMknaSFS1hPNcCzqm4Q=s64","userId":"17713098623346557830"},"user_tz":300},"id":"VyqPqrPj3Z87","outputId":"60c71f32-f9fa-4321-cf6b-02f280a31a13","colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["import pandas as pd\n","import numpy as np\n","\n","CREATE_PER_CHANGE = False\n","\n","if CREATE_PER_CHANGE or not os.path.isfile('data/train_features_input.csv'):\n","  # load data\n","  data_df = pd.read_csv('data/train_features.csv', index_col=0)\n","\n","  ###\n","  # TODO: handle nans and bad data\n","  ###\n","\n","  # Divide to P[t] and P[t-1]\n","  data_yesterday = data_df[:-1]\n","  data_today = data_df[1:]\n","  data_yesterday.index = data_today.index\n","\n","  # price to percent change\n","  perc_chg_df = (data_today/data_yesterday).apply(lambda row: np.log(row))\n","\n","  # fill 0 for ln(negative) or pt/0 in cash\n","  perc_chg_df = perc_chg_df.replace([np.inf, -np.inf], np.nan)\n","  perc_chg_df = perc_chg_df.fillna(0)\n","\n","  # need to manually add Date column name after to_csv\n","  perc_chg_df.reset_index(drop=True).to_csv('data/train_features_input.csv')\n","  print('nans: {}'.format(perc_chg_df.isna().sum().sum()))\n","\n","else:\n","  perc_chg_df = pd.read_csv('data/train_features_input.csv')\n","\n","perc_chg_df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>A</th>\n","      <th>AAL</th>\n","      <th>AAP</th>\n","      <th>AAPL</th>\n","      <th>ABBV</th>\n","      <th>ABC</th>\n","      <th>ABMD</th>\n","      <th>ABT</th>\n","      <th>ACN</th>\n","      <th>...</th>\n","      <th>XLNX</th>\n","      <th>XOM</th>\n","      <th>XRAY</th>\n","      <th>XRX</th>\n","      <th>XYL</th>\n","      <th>YUM</th>\n","      <th>ZBH</th>\n","      <th>ZION</th>\n","      <th>ZTS</th>\n","      <th>Cash</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>-0.009325</td>\n","      <td>0.005207</td>\n","      <td>-0.004531</td>\n","      <td>-0.009558</td>\n","      <td>0.006853</td>\n","      <td>0.003322</td>\n","      <td>-0.019902</td>\n","      <td>-0.002669</td>\n","      <td>-0.005276</td>\n","      <td>...</td>\n","      <td>0.007135</td>\n","      <td>0.004102</td>\n","      <td>-0.025477</td>\n","      <td>-0.007968</td>\n","      <td>0.000263</td>\n","      <td>-0.006887</td>\n","      <td>-0.007345</td>\n","      <td>-0.007747</td>\n","      <td>0.006486</td>\n","      <td>-0.848237</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>-0.018915</td>\n","      <td>-0.000649</td>\n","      <td>-0.013269</td>\n","      <td>-0.028576</td>\n","      <td>-0.018999</td>\n","      <td>-0.008548</td>\n","      <td>-0.006453</td>\n","      <td>0.000223</td>\n","      <td>-0.017028</td>\n","      <td>...</td>\n","      <td>-0.018636</td>\n","      <td>-0.027743</td>\n","      <td>-0.006957</td>\n","      <td>-0.022060</td>\n","      <td>-0.064258</td>\n","      <td>-0.020527</td>\n","      <td>0.036625</td>\n","      <td>-0.038189</td>\n","      <td>-0.006021</td>\n","      <td>-0.408760</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>-0.015700</td>\n","      <td>-0.015620</td>\n","      <td>-0.000703</td>\n","      <td>0.000094</td>\n","      <td>-0.004962</td>\n","      <td>0.005448</td>\n","      <td>-0.025684</td>\n","      <td>-0.011421</td>\n","      <td>-0.007239</td>\n","      <td>...</td>\n","      <td>-0.014475</td>\n","      <td>-0.005330</td>\n","      <td>-0.012488</td>\n","      <td>-0.013472</td>\n","      <td>-0.005898</td>\n","      <td>-0.012351</td>\n","      <td>-0.008513</td>\n","      <td>-0.038942</td>\n","      <td>-0.009804</td>\n","      <td>0.918269</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.013185</td>\n","      <td>-0.000566</td>\n","      <td>0.021261</td>\n","      <td>0.013925</td>\n","      <td>0.039621</td>\n","      <td>0.019763</td>\n","      <td>0.031333</td>\n","      <td>0.008075</td>\n","      <td>0.020772</td>\n","      <td>...</td>\n","      <td>0.000356</td>\n","      <td>0.010082</td>\n","      <td>0.025588</td>\n","      <td>0.012727</td>\n","      <td>0.007856</td>\n","      <td>0.032601</td>\n","      <td>0.024566</td>\n","      <td>0.009311</td>\n","      <td>0.020433</td>\n","      <td>-0.220184</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.029534</td>\n","      <td>0.012187</td>\n","      <td>0.008727</td>\n","      <td>0.037703</td>\n","      <td>0.010404</td>\n","      <td>0.002281</td>\n","      <td>0.044078</td>\n","      <td>0.020346</td>\n","      <td>0.015134</td>\n","      <td>...</td>\n","      <td>0.022033</td>\n","      <td>0.016508</td>\n","      <td>0.024389</td>\n","      <td>0.027158</td>\n","      <td>0.007518</td>\n","      <td>0.017270</td>\n","      <td>0.010561</td>\n","      <td>0.014086</td>\n","      <td>0.015281</td>\n","      <td>-0.289325</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 507 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0        A       AAL       AAP      AAPL      ABBV       ABC   \\\n","0           0 -0.009325  0.005207 -0.004531 -0.009558  0.006853  0.003322   \n","1           1 -0.018915 -0.000649 -0.013269 -0.028576 -0.018999 -0.008548   \n","2           2 -0.015700 -0.015620 -0.000703  0.000094 -0.004962  0.005448   \n","3           3  0.013185 -0.000566  0.021261  0.013925  0.039621  0.019763   \n","4           4  0.029534  0.012187  0.008727  0.037703  0.010404  0.002281   \n","\n","      ABMD       ABT       ACN   ...     XLNX       XOM      XRAY       XRX   \\\n","0 -0.019902 -0.002669 -0.005276  ...  0.007135  0.004102 -0.025477 -0.007968   \n","1 -0.006453  0.000223 -0.017028  ... -0.018636 -0.027743 -0.006957 -0.022060   \n","2 -0.025684 -0.011421 -0.007239  ... -0.014475 -0.005330 -0.012488 -0.013472   \n","3  0.031333  0.008075  0.020772  ...  0.000356  0.010082  0.025588  0.012727   \n","4  0.044078  0.020346  0.015134  ...  0.022033  0.016508  0.024389  0.027158   \n","\n","       XYL       YUM       ZBH      ZION       ZTS       Cash  \n","0  0.000263 -0.006887 -0.007345 -0.007747  0.006486 -0.848237  \n","1 -0.064258 -0.020527  0.036625 -0.038189 -0.006021 -0.408760  \n","2 -0.005898 -0.012351 -0.008513 -0.038942 -0.009804  0.918269  \n","3  0.007856  0.032601  0.024566  0.009311  0.020433 -0.220184  \n","4  0.007518  0.017270  0.010561  0.014086  0.015281 -0.289325  \n","\n","[5 rows x 507 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xxlKhjXk3Z8-"},"source":["## Base csv Loader"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SGv9QAnC3Z8_","colab":{}},"source":["# used by LSTM Dataloader\n","class DatasetLoader():\n","    def __init__(self, data_dir, dataset_name):\n","        dataset_path = '%s/%s.csv' % (data_dir, dataset_name)\n","        self.data_df = pd.read_csv(dataset_path, index_col=0)\n","\n","    # get dataframe or numpy array.\n","    # can sample number of stocks (columns) and limit number of days (rows).\n","    # can also return plot figure with stock prices over time\n","    def get_data(self, limit_days=None, exclude_days=None,\n","                 test_split_days=0, random_state=1, as_numpy=True, plot=False,\n","                 dropna=True, drop_test=False):\n","\n","        data_ret = self.data_df\n","\n","        if 'Date' in data_ret:\n","            # we don't need a separate date col (it's ok if it's the index)\n","            data_ret = self.data_df.drop(['Date'], axis=1)\n","\n","        if limit_days:\n","            # optional limit to latest n days\n","            data_ret = data_ret.tail(limit_days).reset_index(drop=True, inplace=False)\n","\n","        if exclude_days:\n","            # optional exclusion of latest n days\n","            data_ret = data_ret.head(data_ret.shape[0] - exclude_days).reset_index(drop=True, inplace=False)\n","\n","        if dropna:\n","            # optional drop of NaN columns\n","            data_ret = data_ret.dropna(axis=1, how='any') # drop cols/stocks with NA prices in selected day range\n","\n","        # training data\n","        train_data = data_ret\n","        if drop_test:\n","            # optional drop test data from training data\n","            train_data = data_ret[:-test_split_days]\n","            \n","        # test data\n","        test_data = data_ret[-test_split_days:]\n","\n","        if as_numpy:\n","            # optional return numpy\n","            train_data = train_data.to_numpy()\n","            test_data = test_data.to_numpy()\n","\n","        return train_data, test_data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uE9UVrNI3Z9B"},"source":["## LSTM Prices Dataloader"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3bbz9vXE3Z9C","colab":{}},"source":["# Adapted from DV360 challenge dataloader class\n","\n","from datetime import datetime\n","import pandas as pd\n","import numpy as np\n","from random import shuffle\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Sampler\n","\n","\n","class SubsetSampler(Sampler):\n","    \"\"\"Samples elements sequentially from a given list of indices, without replacement.\n","\n","    Arguments:\n","        indices (sequence): a sequence of indices\n","    \"\"\"\n","\n","    def __init__(self, indices):\n","        self.indices = indices\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in range(len(self.indices)))\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def update_indices(self, indices):\n","        self.indices = indices\n","\n","\n","class FuturePricesLoader(DataLoader):\n","\n","    def __init__(self, phase, batch_size, data_dir, dataset_name, past_prices_lookback_window=30,\n","                 target_size=None, limit_days=None, exclude_days=None, random_state=1, history_number=2):\n","\n","        self.futureprices = FuturePrices(phase,\n","                                         data_dir=data_dir,\n","                                         dataset_name=dataset_name,\n","                                         past_prices_lookback_window=past_prices_lookback_window,                 \n","                                         target_size=target_size,\n","                                         limit_days=limit_days,\n","                                         exclude_days=exclude_days,\n","                                         random_state=random_state,\n","                                         history_number=history_number)\n","\n","        sampler = SubsetSampler(self.futureprices.indices)\n","        \n","        # TODO: can make this config driven\n","        num_workers = 1\n","\n","        self.data_dim = self.futureprices.dataframe.shape\n","\n","        super().__init__(dataset=self.futureprices,\n","                         batch_size=batch_size,\n","                         sampler=sampler,\n","                         num_workers=num_workers)\n","\n","    def add_day(self, day_prices):\n","        self.futureprices.add_day(day_prices)\n","        self.sampler.update_indices(self.futureprices.indices)\n","\n","\n","class FuturePrices(object):\n","    def __init__(self, phase, data_dir, dataset_name, past_prices_lookback_window=30,\n","                 target_size=None, limit_days=None, exclude_days=None, random_state=1, history_number=2):\n","\n","        self.past_prices_lookback_window = past_prices_lookback_window\n","                \n","        self.shuffle = False # TODO: can make these config driven or phase dependent (i.e. train, test):\n","        \n","        self.history_number = history_number\n","        if limit_days is not None:\n","            # for testing need to pad days to accommodate for historic number window\n","            limit_days = limit_days + self.history_number + 1\n","\n","        #### reading in dataframe from csv #####\n","        base_dataset_loader = DatasetLoader(data_dir, dataset_name)\n","        self.dataframe, _ = base_dataset_loader.get_data(limit_days=limit_days,\n","                                                         exclude_days=exclude_days,\n","                                                         random_state=random_state,\n","                                                         as_numpy=False)\n","\n","        # first target_size columns will be picked as the target if specified\n","        self.target_size = target_size if target_size else self.dataframe.shape[1]\n","\n","        self.sequence_length = self.history_number\n","        max_temporal_history = self.sequence_length\n","\n","        # we remove window+temporal history from start so we always have a full window\n","        # we remove one from the end so that target/next_prices doesn't index out of bounds\n","        self.indices = self.dataframe.iloc[self.past_prices_lookback_window+max_temporal_history:-2].index.tolist()\n","        self.phase = phase\n","\n","        #### phase specific manipulation #####\n","        if phase == 'train':\n","            pass\n","\n","        elif phase == 'validation':\n","            pass\n","\n","        elif phase == 'test':\n","            # for test phase we want to append new predicted days\n","            self.indices = self.dataframe.iloc[self.past_prices_lookback_window+max_temporal_history:].index.tolist()\n","\n","        if self.shuffle:\n","            shuffle(self.indices)\n","\n","        print('Phase:', phase, '# of data:', len(self.indices))\n","        \n","        # data transforms - TODO: can compose more data transforms\n","        self.past_prices_transform = transforms.Compose([\n","                transforms.ToTensor()\n","            ])\n","\n","        self.next_prices_transform = transforms.Compose([\n","                transforms.ToTensor()\n","            ])\n","\n","    def add_day(self, day_prices):\n","        # update dataframe with new day and new index\n","        new_row = dict(zip(self.dataframe.columns, day_prices))\n","        current_max_index = self.dataframe.index.max()\n","        # new_index = (pd.Timestamp(current_max_index) + pd.DateOffset(days=1)).strftime('%d/%m/%Y')\n","        new_index = current_max_index + 1\n","        new_row_series = pd.Series(new_row, name=new_index)\n","\n","        self.dataframe = self.dataframe.append(new_row_series)\n","\n","        # update indices\n","        # remove oldest index and append new one (we don't want to process first day again)\n","        self.indices = self.indices[1:]\n","        self.indices.append(new_index)\n","\n","    def __getitem__(self, index):\n","        inputs = {}\n","        labels = {}\n","        window_start = index - self.past_prices_lookback_window\n","\n","        for i in range(self.sequence_length):\n","            inputs[i] = {}\n","            past_prices_img = self.dataframe.iloc[window_start-i+1:index-i+1].reset_index(drop=True, inplace=False).to_numpy()\n","            past_prices_img = self.past_prices_transform(past_prices_img)\n","            inputs[i]['past_prices'] = past_prices_img\n","\n","        if self.phase in ('train', 'validation'):\n","            # training/validation labels\n","            next_prices = self.dataframe.iloc[index+1, :self.target_size].to_numpy().reshape(1, -1)\n","            next_prices = self.next_prices_transform(next_prices)\n","\n","            labels['next_prices'] = next_prices\n","        else:\n","            # no labels for test\n","            labels['next_prices'] = np.empty((1, self.target_size))\n","        \n","        return inputs, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TrvelUyT3Z9E"},"source":["## Models"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LyKIKKDL3Z9F","colab":{}},"source":["# Adapted from DV360 LSTM model\n","\n","from torchvision import models\n","import torch.nn as nn\n","import torch\n","\n","\n","class PricePredictionModel(nn.Module):\n","    def __init__(self, input_size=506, output_size=506, hidden_size=128,\n","                 num_layers=3):\n","        super(PricePredictionModel, self).__init__()\n","        final_concat_size = 0\n","\n","        # CNN\n","        cnn = models.resnet34(pretrained=True)\n","        feats = list(cnn.children())[:-1]\n","        feats[0] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        self.features = nn.Sequential(*feats)\n","        self.intermediate = nn.Sequential(nn.Linear(\n","            cnn.fc.in_features, input_size),\n","            nn.ReLU())\n","        final_concat_size += input_size\n","\n","        # LSTM\n","        self.lstm = nn.LSTM(input_size=input_size,\n","                            hidden_size=hidden_size,\n","                            num_layers=num_layers,\n","                            batch_first=False)\n","        final_concat_size += hidden_size\n","\n","        # Prices Regressor\n","        self.predict_prices = nn.Sequential(\n","            nn.Linear(final_concat_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, int(hidden_size / 2)),\n","            nn.ReLU(),\n","            nn.Linear(int(hidden_size / 2), output_size)\n","        )\n","\n","    def forward(self, data_input):\n","        module_outputs = []\n","        lstm_i = []\n","        # Loop through temporal sequence of price \"images\" and pass through the cnn.\n","\n","        for idx, v in data_input.items():\n","            x = self.features(v['past_prices'])\n","            x = x.view(x.size(0), -1)\n","            x = self.intermediate(x)\n","            lstm_i.append(x)\n","            # feed the current output directly into the regression network.\n","            if idx == 0:\n","                module_outputs.append(x)\n","\n","        # Feed temporal outputs of CNN into LSTM\n","        i_lstm, _ = self.lstm(torch.stack(lstm_i))\n","        module_outputs.append(i_lstm[-1])\n","\n","        # Concatenate current image CNN output and LSTM output.\n","        x_cat = torch.cat(module_outputs, dim=-1)\n","\n","        # Feed concatenated outputs into the regession networks.\n","        prediction = {'next_prices': torch.squeeze(self.predict_prices(x_cat))}\n","        return prediction"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x-00zUUG3Z9H"},"source":["## Train method"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6b9cdM303Z9H","colab":{}},"source":["from torch import nn, optim\n","import time\n","import torch\n","\n","def train(model, lr, train_loader, validation_loader, epochs, device, log_interval):\n","\n","    # Loss and optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(0, epochs):\n","        # epoch training\n","        \n","        start_epoch_train = time.time()\n","        epoch_prices_losses = []        \n","        model.train()\n","        running_prices_loss = 0.0\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","\n","            # convert data and labels to device [cpu, cuda]\n","            for k, v in data.items():\n","                for k2, v2 in v.items():\n","                    data[k][k2] = v2.float().to(device)\n","            for k, v in target.items():\n","                target[k] = v.float().to(device)\n","\n","            optimizer.zero_grad()\n","            prediction = model(data)\n","\n","            # loss, backprop, optimize\n","            prices_loss = criterion(prediction['next_prices'], target['next_prices'].squeeze())\n","            combined_loss = prices_loss\n","            combined_loss.backward()\n","\n","            optimizer.step()\n","\n","            running_prices_loss += prices_loss.item()\n","            if batch_idx > 0 and batch_idx % log_interval == 0:\n","                # print avg batch statistics\n","                avg_batch_prices_loss = running_prices_loss / log_interval\n","                epoch_prices_losses.append(avg_batch_prices_loss)\n","                print('[epoch: %d, batch:  %5d] prices loss: %.5f' % (epoch + 1, batch_idx + 1, avg_batch_prices_loss))\n","                running_prices_loss = 0.0\n","\n","        if len(epoch_prices_losses) > 0 and len(epoch_prices_losses) > 0:\n","            # print avg epoch statistics\n","            epoch_prices_loss = sum(epoch_prices_losses) / len(epoch_prices_losses)\n","            print('[avg train loss epoch %d] prices loss %.5f' % (epoch, epoch_prices_loss))\n","        else:\n","            print('0 epoch losses for training')\n","\n","        end_epoch_train = time.time()\n","        epoch_elapsed = end_epoch_train - start_epoch_train\n","        print('epoch %d: %f elapsed' % (epoch+1, epoch_elapsed))\n","\n","        if validation_loader:\n","            # epoch validation\n","            \n","            model.eval()\n","            with torch.no_grad():\n","                epoch_validation_prices_losses = []\n","                for batch_idx, (data, target) in enumerate(validation_loader):\n","                    \n","                    # convert data and labels to device [cpu, cuda]\n","                    for k, v in data.items():\n","                        for k2, v2 in v.items():\n","                            data[k][k2] = v2.float().to(device)\n","                    for k, v in target.items():\n","                        target[k] = v.float().to(device)\n","\n","                    prediction = model(data)\n","\n","                    prices_loss = criterion(prediction['next_prices'], target['next_prices'].squeeze())\n","\n","                    epoch_validation_prices_losses.append(prices_loss.item())\n","\n","                if len(epoch_validation_prices_losses) > 0:\n","                    epoch_validation_prices_loss = sum(epoch_validation_prices_losses) / len(epoch_validation_prices_losses)\n","                    print('[avg validation loss epoch %d] prices loss %.5f' % (epoch, epoch_validation_prices_loss))\n","                else:\n","                    print('0 epoch losses for validation')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x_QFv1TA3Z9K"},"source":["## Test method"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9jHyESJh3Z9K","colab":{}},"source":["import torch\n","import pandas as pd\n","\n","\n","# generates a results.csv containing predictions for all existing training days + new test days for predict_days\n","def test(model, predict_days, train_loader, test_loader, device):\n","\n","    initial_data_shape = test_loader.futureprices.dataframe.shape\n","    # init output file\n","    output_path = 'prediction/results.csv'\n","    columns = train_loader.futureprices.dataframe.columns\n","    pd.DataFrame([], columns=columns).to_csv(output_path)\n","\n","    output_interval = 50\n","    last_output_index = 0\n","    num_batches = len(train_loader)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        # generate predictions for existing days and save to results.csv in batches\n","        output = []\n","        for batch_idx, (data, _) in enumerate(train_loader):\n","            \n","            # convert data to device [cpu, cuda]\n","            for k, v in data.items():\n","                for k2, v2 in v.items():\n","                    data[k][k2] = v2.float().to(device)\n","\n","            # predict and append to output\n","            prediction = model(data)\n","            output.append(prediction['next_prices'].detach().cpu().numpy())\n","\n","            if (batch_idx+1) % output_interval == 0 or (batch_idx+1) == num_batches:\n","                # every output_interval iterations append predictions to results.csv file on disk\n","                print('predicting training day t+1 {}/{}...'.format(\n","                    batch_idx+1, num_batches))\n","\n","                pd.DataFrame(output, columns=columns, index=range(last_output_index, batch_idx+1))\\\n","                    .to_csv(output_path,\n","                            header=False,\n","                            mode='a')\n","                last_output_index = batch_idx + 1\n","                output = []\n","\n","        # predict new days and save to results.csv\n","        for day in range(predict_days):\n","            for batch_idx, (data, _) in enumerate(test_loader):\n","                \n","                # convert data to device [cpu, cuda]\n","                for k, v in data.items():\n","                    for k2, v2 in v.items():\n","                        data[k][k2] = v2.float().to(device)\n","\n","                # predict a new day and append it to the test dataloader\n","                # so it can later be used to predict the next day\n","                prediction = model(data)\n","                test_loader.add_day(prediction['next_prices'].cpu().numpy())\n","\n","                if (day+1) % 5 == 0 or (day+1) == predict_days:\n","                    print('predicting new day {}/{}...'.format(day+1, predict_days))\n","\n","        print('Finished | initial data shape: {} | final data shape: {}'.format(\n","            initial_data_shape, test_loader.futureprices.dataframe.shape))\n","\n","    # append new days to results.csv file on disk\n","    print('Saving results to \"{}\"...'.format(output_path))\n","    output_df = test_loader.futureprices.dataframe.iloc[-predict_days:]\n","    output_df.index = range(last_output_index, last_output_index+len(output_df))\n","    output_df.to_csv(output_path, header=False, mode='a')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lt5LXPnX3Z9M"},"source":["## Args"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B0bbN_H83Z9N","colab":{}},"source":["force_cpu = False\n","data_dir = 'data' # directory in which the data is\n","dataset_name = 'train_new_percent_change' # name of csv file in data_dir (excluding .csv extension)\n","modes = ['train', 'test'] # train, test or train-and-test\n","\n","batch_size = 1 # training batch size - Note haven't tested with more than 1, might only work with 1 on the 'test' phase\n","epochs = 5 # number of training epochs\n","lr = 0.0001 # learning rate\n","\n","days_lookback_window = 20 # LSTM sliding window size\n","history_number = 30 # number of sliding temporal windows to consider at each iteration (length of LSTM in time)\n","num_layers = 1 # number of layers of LSTM model\n","hidden_size = 512 # int(input_size / 4)\n","\n","test_predict_days = 30 # number of new days to predict\n","log_interval = 50 # interval of batches to print statistics for during training\n","\n","limit_days = None # optional: integer limit to limit_days final days of the dataset\n","val_days = None # optional: integer validation set size in days (taken from end of dataset)\n","target_size = None # optional: integer number of columns to have in final prediction. If not specified will be identical to input number of columns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8KqtC4kY3Z9P"},"source":["## Main runner"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":310056,"status":"ok","timestamp":1575743790924,"user":{"displayName":"Ziyan Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDy4jjH-H5uFROlqAKrvnbKMknaSFS1hPNcCzqm4Q=s64","userId":"17713098623346557830"},"user_tz":300},"id":"EnA_pfuo3Z9Q","outputId":"a673fda2-3eac-4f21-ccd3-87a0f8a2cd8f","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import torch\n","\n","###\n","# Run this to generate results.csv with target_size columns and K rows where:\n","# K = min(training data, limit_days) - val_days - days_lookback_window - history_number + test_predict_days\n","# The final test_predict_days rows are the new generated prediction for the new days\n","###\n","\n","# SETUP DEVICE #\n","device_type = 'cuda' if not force_cpu and torch.cuda.is_available() else 'cpu'\n","device = torch.device(device_type)\n","print('device is: ', device)\n","# END SETUP DEVICE #\n","\n","# SETUP DATALOADERS #\n","\n","val_data_dim = None\n","test_data_dim = None\n","validation_loader = None\n","test_loader = None\n","\n","train_loader = FuturePricesLoader('train', batch_size, data_dir, dataset_name,\n","                                      days_lookback_window,\n","                                      target_size=target_size,\n","                                      limit_days=limit_days,\n","                                      exclude_days=val_days,\n","                                      history_number=history_number)\n","\n","train_data_dim = train_loader.data_dim\n","\n","if val_days and val_days > 0:\n","    validation_loader = FuturePricesLoader('validation', batch_size, data_dir, dataset_name,\n","                                          days_lookback_window,\n","                                          target_size=target_size,\n","                                          limit_days=val_days,\n","                                          history_number=history_number)\n","    val_data_dim = validation_loader.data_dim\n","\n","\n","if 'test' in modes:\n","    test_loader = FuturePricesLoader('test', batch_size, data_dir, dataset_name,\n","                                          days_lookback_window,\n","                                          target_size=target_size,\n","                                          limit_days=days_lookback_window,\n","                                          history_number=history_number)\n","\n","    test_data_dim = test_loader.data_dim\n","\n","\n","output_size = train_loader.futureprices.target_size if train_loader else test_loader.futureprices.target_size\n","\n","\n","validation_dataloader = None\n","# END SETUP DATALOADERS #\n","\n","params = {\n","    'lr': lr,\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'log_interval': log_interval,\n","    'device': device_type,\n","    'train_data_shape': train_data_dim,\n","    'validation_data_shape': val_data_dim,\n","    'test_data_shape': test_data_dim,\n","    'test_predict_days': test_predict_days,\n","    'target_size': target_size\n","}\n","\n","print('running with params: {}'.format(params))\n","\n","# SETUP MODEL #\n","assert (train_data_dim or test_data_dim)\n","input_size = train_data_dim[1] if train_data_dim else test_data_dim[1]\n","\n","model = PricePredictionModel(input_size=input_size,\n","                             output_size=output_size,\n","                             hidden_size=hidden_size,\n","                             num_layers=num_layers)\n","\n","if device_type == 'cuda':\n","    model.cuda()\n","# END SETUP MODEL #\n","\n","\n","if 'train' in modes:\n","    print('--Started training--')\n","    train(model, lr, train_loader, validation_loader, epochs, device, log_interval)\n","    print('--Finished training--')\n","\n","if 'test' in modes:\n","    print('--Started testing--')\n","    test(model, test_predict_days, train_loader, test_loader, device)\n","    print('--Finished testing--')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["device is:  cuda\n","Phase: train # of data: 838\n","Phase: test # of data: 1\n","running with params: {'lr': 0.0001, 'batch_size': 1, 'epochs': 5, 'log_interval': 50, 'device': 'cuda', 'train_data_shape': (890, 506), 'validation_data_shape': None, 'test_data_shape': (51, 506), 'test_predict_days': 30, 'target_size': None}\n","--Started training--\n","[epoch: 1, batch:     51] prices loss: 0.00127\n","[epoch: 1, batch:    101] prices loss: 0.00044\n","[epoch: 1, batch:    151] prices loss: 0.00107\n","[epoch: 1, batch:    201] prices loss: 0.00046\n","[epoch: 1, batch:    251] prices loss: 0.00054\n","[epoch: 1, batch:    301] prices loss: 0.00031\n","[epoch: 1, batch:    351] prices loss: 0.00030\n","[epoch: 1, batch:    401] prices loss: 0.00022\n","[epoch: 1, batch:    451] prices loss: 0.00032\n","[epoch: 1, batch:    501] prices loss: 0.00020\n","[epoch: 1, batch:    551] prices loss: 0.00015\n","[epoch: 1, batch:    601] prices loss: 0.00019\n","[epoch: 1, batch:    651] prices loss: 0.00020\n","[epoch: 1, batch:    701] prices loss: 0.00019\n","[epoch: 1, batch:    751] prices loss: 0.00018\n","[epoch: 1, batch:    801] prices loss: 0.00038\n","[avg train loss epoch 0] prices loss 0.00040\n","epoch 1: 423.002289 elapsed\n","[epoch: 2, batch:     51] prices loss: 0.00059\n","[epoch: 2, batch:    101] prices loss: 0.00039\n","[epoch: 2, batch:    151] prices loss: 0.00105\n","[epoch: 2, batch:    201] prices loss: 0.00045\n","[epoch: 2, batch:    251] prices loss: 0.00053\n","[epoch: 2, batch:    301] prices loss: 0.00030\n","[epoch: 2, batch:    351] prices loss: 0.00030\n","[epoch: 2, batch:    401] prices loss: 0.00022\n","[epoch: 2, batch:    451] prices loss: 0.00031\n","[epoch: 2, batch:    501] prices loss: 0.00020\n","[epoch: 2, batch:    551] prices loss: 0.00014\n","[epoch: 2, batch:    601] prices loss: 0.00018\n","[epoch: 2, batch:    651] prices loss: 0.00020\n","[epoch: 2, batch:    701] prices loss: 0.00019\n","[epoch: 2, batch:    751] prices loss: 0.00018\n","[epoch: 2, batch:    801] prices loss: 0.00038\n","[avg train loss epoch 1] prices loss 0.00035\n","epoch 2: 481.384028 elapsed\n","[epoch: 3, batch:     51] prices loss: 0.00059\n","[epoch: 3, batch:    101] prices loss: 0.00039\n","[epoch: 3, batch:    151] prices loss: 0.00104\n","[epoch: 3, batch:    201] prices loss: 0.00045\n","[epoch: 3, batch:    251] prices loss: 0.00052\n","[epoch: 3, batch:    301] prices loss: 0.00030\n","[epoch: 3, batch:    351] prices loss: 0.00029\n","[epoch: 3, batch:    401] prices loss: 0.00022\n","[epoch: 3, batch:    451] prices loss: 0.00031\n","[epoch: 3, batch:    501] prices loss: 0.00020\n","[epoch: 3, batch:    551] prices loss: 0.00014\n","[epoch: 3, batch:    601] prices loss: 0.00018\n","[epoch: 3, batch:    651] prices loss: 0.00020\n","[epoch: 3, batch:    701] prices loss: 0.00019\n","[epoch: 3, batch:    751] prices loss: 0.00018\n","[epoch: 3, batch:    801] prices loss: 0.00037\n","[avg train loss epoch 2] prices loss 0.00035\n","epoch 3: 423.846644 elapsed\n","[epoch: 4, batch:     51] prices loss: 0.00058\n","[epoch: 4, batch:    101] prices loss: 0.00038\n","[epoch: 4, batch:    151] prices loss: 0.00104\n","[epoch: 4, batch:    201] prices loss: 0.00045\n","[epoch: 4, batch:    251] prices loss: 0.00052\n","[epoch: 4, batch:    301] prices loss: 0.00030\n","[epoch: 4, batch:    351] prices loss: 0.00029\n","[epoch: 4, batch:    401] prices loss: 0.00022\n","[epoch: 4, batch:    451] prices loss: 0.00031\n","[epoch: 4, batch:    501] prices loss: 0.00020\n","[epoch: 4, batch:    551] prices loss: 0.00014\n","[epoch: 4, batch:    601] prices loss: 0.00018\n","[epoch: 4, batch:    651] prices loss: 0.00020\n","[epoch: 4, batch:    701] prices loss: 0.00019\n","[epoch: 4, batch:    751] prices loss: 0.00018\n","[epoch: 4, batch:    801] prices loss: 0.00037\n","[avg train loss epoch 3] prices loss 0.00035\n","epoch 4: 423.812854 elapsed\n","[epoch: 5, batch:     51] prices loss: 0.00058\n","[epoch: 5, batch:    101] prices loss: 0.00038\n","[epoch: 5, batch:    151] prices loss: 0.00104\n","[epoch: 5, batch:    201] prices loss: 0.00045\n","[epoch: 5, batch:    251] prices loss: 0.00052\n","[epoch: 5, batch:    301] prices loss: 0.00030\n","[epoch: 5, batch:    351] prices loss: 0.00029\n","[epoch: 5, batch:    401] prices loss: 0.00021\n","[epoch: 5, batch:    451] prices loss: 0.00031\n","[epoch: 5, batch:    501] prices loss: 0.00019\n","[epoch: 5, batch:    551] prices loss: 0.00014\n","[epoch: 5, batch:    601] prices loss: 0.00018\n","[epoch: 5, batch:    651] prices loss: 0.00019\n","[epoch: 5, batch:    701] prices loss: 0.00019\n","[epoch: 5, batch:    751] prices loss: 0.00018\n","[epoch: 5, batch:    801] prices loss: 0.00037\n","[avg train loss epoch 4] prices loss 0.00035\n","epoch 5: 423.689714 elapsed\n","--Finished training--\n","--Started testing--\n","predicting training day t+1 50/838...\n","predicting training day t+1 100/838...\n","predicting training day t+1 150/838...\n","predicting training day t+1 200/838...\n","predicting training day t+1 250/838...\n","predicting training day t+1 300/838...\n","predicting training day t+1 350/838...\n","predicting training day t+1 400/838...\n","predicting training day t+1 450/838...\n","predicting training day t+1 500/838...\n","predicting training day t+1 550/838...\n","predicting training day t+1 600/838...\n","predicting training day t+1 650/838...\n","predicting training day t+1 700/838...\n","predicting training day t+1 750/838...\n","predicting training day t+1 800/838...\n","predicting training day t+1 838/838...\n","predicting new day 5/30...\n","predicting new day 10/30...\n","predicting new day 15/30...\n","predicting new day 20/30...\n","predicting new day 25/30...\n","predicting new day 30/30...\n","Finished | initial data shape: (51, 506) | final data shape: (81, 506)\n","Saving results to \"prediction/results.csv\"...\n","--Finished testing--\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mgqQ96aJ3Z9S"},"source":["# 2. Conic weight optimizer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"slYXdeMX3Z9S"},"source":["## Helper methods"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9DkIrn2E3Z9T","colab":{}},"source":["import numpy as np\n","from scipy.optimize import minimize\n","import pandas as pd\n","\n","\n","# conic optimization of price change covariance to weights using SLSQP method\n","def optimize(num_stocks, cov, expected_val, bounds):\n","#     print(num_stocks, cov.shape, expected_val.shape)\n","    init_guess = np.ones(num_stocks) * (1.0 / num_stocks)\n","    expected_val = expected_val.values.reshape((num_stocks, 1))\n","    \n","#     print('1', np.matmul(init_guess.T,np.matmul(cov,init_guess)).shape)\n","#     print('2', np.matmul(init_guess,expected_val).shape, init_guess.shape, expected_val.shape)\n","    \n","    weights = minimize(\n","        lambda w: (np.matmul(w.T,np.matmul(cov,w)) - np.matmul(w.T,expected_val)), \n","        init_guess,\n","        method='SLSQP', \n","        options={'disp': False}, \n","        constraints=({'type': 'eq', 'fun': lambda w: 1.0 - np.sum(w)}), \n","        bounds=bounds\n","    )\n","    return weights.x\n","\n","\n","# multiprocessing approach to optimization \n","# (each row takes 30 seconds to optimize, \n","#  so multithreading is crucial)\n","# modifies w_ret\n","def handle_optimization(i, bounds, lookback_window, prices, num_stocks, num_days):\n","    # get lookback of k days from real prices and lookahead of 1\n","    train = prices.iloc[i-(lookback_window-1):i+1]\n","    print(train.shape)\n","    print(\"optimizing row {}/{}...\".format(i-(lookback_window-1), num_days - (lookback_window-1)))\n","    cov = train.cov()\n","    expected_val = train.mean()\n","    print('expected_val', expected_val.shape)\n","    test = optimize(num_stocks, cov.values, expected_val, bounds)\n","    return (i-(lookback_window-1), test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F7fgbBt83Z9V"},"source":["## Args"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_JGi-zcs3Z9W","colab":{}},"source":["from datetime import datetime\n","\n","curr_time = datetime.now().strftime('%H%M')\n","\n","# input_path = f'prediction/results_{curr_time}.csv' # full path to price change csv\n","# output_path = f'prediction/weights_{curr_time}.csv' # full output path for weights csv\n","\n","input_path = 'prediction/results.csv' # full path to price change csv\n","output_path = 'prediction/weights.csv' # full output path for weights csv\n","weight_min = -0.5 # minimum weights range bound\n","weight_max = 2 # maximum weights range bound\n","lookback_window = 5 # lookback window in days for optimization\n","num_threads = 4 # number of concurrent threads to use (upper bounded by available cpus)\n","target_days = test_predict_days # number of desired output rows"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AtrhpsM63Z9Y"},"source":["## Main runner"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1963,"status":"error","timestamp":1575747906029,"user":{"displayName":"Ziyan Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDy4jjH-H5uFROlqAKrvnbKMknaSFS1hPNcCzqm4Q=s64","userId":"17713098623346557830"},"user_tz":300},"id":"a1Yz_upI3Z9Z","outputId":"5828476f-0d29-46d6-ab45-e9107e80cb84","colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["import pandas as pd\n","import numpy as np\n","from scipy.optimize import Bounds\n","from functools import partial\n","import argparse\n","import multiprocessing as mp\n","\n","num_threads = min(num_threads, mp.cpu_count())\n","\n","###\n","# This conic optimizer is used to optimize a matrix of price changes \n","# into a matrix of portfolio weights in a range.\n","# We only want to optimize weights for the new predicted days in the \n","# conic optimizer.\n","# For each row the optimizer looks at today and a window of k days in the past\n","###\n","\n","prices = pd.read_csv(input_path, index_col=0)\n","\n","# consider only final target_days + the lookback window\n","prices = prices[-(target_days + (lookback_window - 1)):]\n","\n","num_days = prices.shape[0]\n","num_stocks = prices.shape[1]\n","\n","thread_pool = mp.Pool(num_threads) # concurrency\n","\n","w_ret = np.zeros((target_days, prices.shape[1]))\n","print('output shape: {}'.format(w_ret.shape))\n","# bounds for weights. -.5/2 default\n","bounds = Bounds([weight_min]*num_stocks, [weight_max]*num_stocks)\n","\n","# each row can take 30-60 seconds to optimize, so multithreading can be helpful\n","multithread_partial = partial(\n","    handle_optimization,\n","    bounds=bounds,\n","    lookback_window=lookback_window,\n","    prices=prices,\n","    num_stocks=num_stocks,\n","    num_days=num_days\n",")\n","\n","# print(range((lookback_window-1), num_days))\n","optimal_weights_unsorted = thread_pool.map(\n","    multithread_partial, \n","    range((lookback_window-1), num_days)\n",")\n","\n","# realign output into matrix by indices\n","for i, w in sorted(optimal_weights_unsorted, key=lambda pair: pair[0]):\n","    w_ret[i, :] = w\n","\n","print('Saving optimal weights to \"{}\"'.format(output_path))\n","weights_df = pd.DataFrame(w_ret, columns=prices.columns)\n","weights_df.to_csv(output_path, index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(5, 506)\n","(5, 506)\n","(5, 506)\n","optimizing row 2/30...\n","(5, 506)\n","optimizing row 0/30...\n","optimizing row 4/30...\n","optimizing row 6/30...\n","expected_val (506,)\n","expected_val (506,)\n","expected_val (506,)\n","expected_val (506,)\n","output shape: (30, 506)\n","(5, 506)\n","optimizing row 1/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 7/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 5/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 3/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 8/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 10/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 12/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 14/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 9/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 13/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 11/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 15/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 16/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 18/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 20/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 22/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 17/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 19/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 21/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 23/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 24/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 26/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 28/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 27/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 29/30...\n","expected_val (506,)\n","(5, 506)\n","optimizing row 25/30...\n","expected_val (506,)\n","Saving optimal weights to \"prediction/weights.csv\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"89R32-VC3Z9b","colab":{},"outputId":"d8b71d50-4734-4ca6-bf4d-8867e15af904"},"source":["weights_df = pd.read_csv('prediction/weights.csv')\n","columns = [col.strip() for col in weights_df.columns]\n","# print(weights_df.head())\n","\n","out = ['Id,Predicted']\n","for ind, row in enumerate(weights_df.itertuples(index=False)):\n","    for col_ind, col in enumerate(row):\n","        out.append(f'{ind}_{columns[col_ind]},{col}')\n","\n","# write to submission file\n","sub_file = 'prediction/submission.csv'\n","with open(sub_file, 'w') as f:\n","    for l in out:\n","        f.write(l + '\\n')\n","print(f'wrote to {sub_file}')\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["wrote to prediction/submission.csv\n"],"name":"stdout"}]}]}
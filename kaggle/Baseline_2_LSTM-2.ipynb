{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baseline_2_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DbMUk6vM3Z83"
      },
      "source": [
        "# Customized baseline model with LSTM price prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jbhC0vnO3Z84"
      },
      "source": [
        "#### Table of contents\n",
        "- LSTM price change prediction\n",
        "- Conic optimizer to transform price change data to optimal portfolio weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kYQEwor3Z86"
      },
      "source": [
        "# 1. LSTM Price Change Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N59hskR-3mOV",
        "outputId": "ed88d656-cd0e-4070-f054-9aa096cd5643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "\"\"\" mount drive \"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "GOOGLE_DIR = '/content/drive/My Drive/DL_Project/Kaggle_New'\n",
        "LOCAL_DIR = '/home/ziyan/Desktop/Deep Learning Project/Kaggle_New'\n",
        "\n",
        "def colab_mount_google_drive():\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    os.chdir(GOOGLE_DIR)\n",
        "    os.listdir()\n",
        "\n",
        "def mount_local_drive():\n",
        "    os.chdir(LOCAL_DIR)\n",
        "    os.listdir()\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    colab_mount_google_drive()\n",
        "    DIR = GOOGLE_DIR\n",
        "    print('Mounted google drive')\n",
        "except ModuleNotFoundError:\n",
        "    mount_local_drive\n",
        "    DIR = LOCAL_DIR\n",
        "    print('Mounted local drive')\n",
        "  \n",
        "print(os.listdir())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted local drive\n",
            "['Baseline_2_LSTM.ipynb', 'cudlfinance.zip', 'Baseline_2_LSTM (3).ipynb', 'sample_submission.csv', 'Baseline_2_LSTM (2).ipynb', 'Baseline_2_LSTM (1).ipynb', 'prediction', 'data', 'train_updated.csv', '.ipynb_checkpoints']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0CxTSWG-3Z87"
      },
      "source": [
        "## Preprocess data\n",
        "We convert price data to log natural price change data by performing:\n",
        "ln(P[t] / P[t-1])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VyqPqrPj3Z87",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CREATE_PER_CHANGE = False\n",
        "\n",
        "if CREATE_PER_CHANGE or not os.path.isfile('data/train_features_input.csv'):\n",
        "  # load data\n",
        "  data_df = pd.read_csv('data/train_features.csv', index_col=0)\n",
        "\n",
        "  ###\n",
        "  # TODO: handle nans and bad data\n",
        "  ###\n",
        "\n",
        "  # Divide to P[t] and P[t-1]\n",
        "  data_yesterday = data_df[:-1]\n",
        "  data_today = data_df[1:]\n",
        "  data_yesterday.index = data_today.index\n",
        "\n",
        "  # price to percent change\n",
        "  perc_chg_df = (data_today/data_yesterday).apply(lambda row: np.log(row))\n",
        "\n",
        "  # fill 0 for ln(negative) or pt/0 in cash\n",
        "  perc_chg_df = perc_chg_df.replace([np.inf, -np.inf], np.nan)\n",
        "  perc_chg_df = perc_chg_df.fillna(0)\n",
        "\n",
        "  # need to manually add Date column name after to_csv\n",
        "  perc_chg_df.reset_index(drop=True).to_csv('data/train_features_input.csv')\n",
        "  print('nans: {}'.format(perc_chg_df.isna().sum().sum()))\n",
        "\n",
        "else:\n",
        "  perc_chg_df = pd.read_csv('data/train_features_input.csv')\n",
        "\n",
        "perc_chg_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xxlKhjXk3Z8-"
      },
      "source": [
        "## Base csv Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SGv9QAnC3Z8_",
        "colab": {}
      },
      "source": [
        "# used by LSTM Dataloader\n",
        "class DatasetLoader():\n",
        "    def __init__(self, data_dir, dataset_name):\n",
        "        dataset_path = '%s/%s.csv' % (data_dir, dataset_name)\n",
        "        self.data_df = pd.read_csv(dataset_path, index_col=0)\n",
        "\n",
        "    # get dataframe or numpy array.\n",
        "    # can sample number of stocks (columns) and limit number of days (rows).\n",
        "    # can also return plot figure with stock prices over time\n",
        "    def get_data(self, limit_days=None, exclude_days=None,\n",
        "                 test_split_days=0, random_state=1, as_numpy=True, plot=False,\n",
        "                 dropna=True, drop_test=False):\n",
        "\n",
        "        data_ret = self.data_df\n",
        "\n",
        "        if 'Date' in data_ret:\n",
        "            # we don't need a separate date col (it's ok if it's the index)\n",
        "            data_ret = self.data_df.drop(['Date'], axis=1)\n",
        "\n",
        "        if limit_days:\n",
        "            # optional limit to latest n days\n",
        "            data_ret = data_ret.tail(limit_days).reset_index(drop=True, inplace=False)\n",
        "\n",
        "        if exclude_days:\n",
        "            # optional exclusion of latest n days\n",
        "            data_ret = data_ret.head(data_ret.shape[0] - exclude_days).reset_index(drop=True, inplace=False)\n",
        "\n",
        "        if dropna:\n",
        "            # optional drop of NaN columns\n",
        "            data_ret = data_ret.dropna(axis=1, how='any') # drop cols/stocks with NA prices in selected day range\n",
        "\n",
        "        # training data\n",
        "        train_data = data_ret\n",
        "        if drop_test:\n",
        "            # optional drop test data from training data\n",
        "            train_data = data_ret[:-test_split_days]\n",
        "            \n",
        "        # test data\n",
        "        test_data = data_ret[-test_split_days:]\n",
        "\n",
        "        if as_numpy:\n",
        "            # optional return numpy\n",
        "            train_data = train_data.to_numpy()\n",
        "            test_data = test_data.to_numpy()\n",
        "\n",
        "        return train_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uE9UVrNI3Z9B"
      },
      "source": [
        "## LSTM Prices Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bbz9vXE3Z9C",
        "colab": {}
      },
      "source": [
        "# Adapted from DV360 challenge dataloader class\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "\n",
        "\n",
        "class SubsetSampler(Sampler):\n",
        "    \"\"\"Samples elements sequentially from a given list of indices, without replacement.\n",
        "\n",
        "    Arguments:\n",
        "        indices (sequence): a sequence of indices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in range(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def update_indices(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "\n",
        "class FuturePricesLoader(DataLoader):\n",
        "\n",
        "    def __init__(self, phase, batch_size, data_dir, dataset_name, past_prices_lookback_window=30,\n",
        "                 target_size=None, limit_days=None, exclude_days=None, random_state=1, history_number=2):\n",
        "\n",
        "        self.futureprices = FuturePrices(phase,\n",
        "                                         data_dir=data_dir,\n",
        "                                         dataset_name=dataset_name,\n",
        "                                         past_prices_lookback_window=past_prices_lookback_window,                 \n",
        "                                         target_size=target_size,\n",
        "                                         limit_days=limit_days,\n",
        "                                         exclude_days=exclude_days,\n",
        "                                         random_state=random_state,\n",
        "                                         history_number=history_number)\n",
        "\n",
        "        sampler = SubsetSampler(self.futureprices.indices)\n",
        "        \n",
        "        # TODO: can make this config driven\n",
        "        num_workers = 1\n",
        "\n",
        "        self.data_dim = self.futureprices.dataframe.shape\n",
        "\n",
        "        super().__init__(dataset=self.futureprices,\n",
        "                         batch_size=batch_size,\n",
        "                         sampler=sampler,\n",
        "                         num_workers=num_workers)\n",
        "\n",
        "    def add_day(self, day_prices):\n",
        "        self.futureprices.add_day(day_prices)\n",
        "        self.sampler.update_indices(self.futureprices.indices)\n",
        "\n",
        "\n",
        "class FuturePrices(object):\n",
        "    def __init__(self, phase, data_dir, dataset_name, past_prices_lookback_window=30,\n",
        "                 target_size=None, limit_days=None, exclude_days=None, random_state=1, history_number=2):\n",
        "\n",
        "        self.past_prices_lookback_window = past_prices_lookback_window\n",
        "                \n",
        "        self.shuffle = False # TODO: can make these config driven or phase dependent (i.e. train, test):\n",
        "        \n",
        "        self.history_number = history_number\n",
        "        if limit_days is not None:\n",
        "            # for testing need to pad days to accommodate for historic number window\n",
        "            limit_days = limit_days + self.history_number + 1\n",
        "\n",
        "        #### reading in dataframe from csv #####\n",
        "        base_dataset_loader = DatasetLoader(data_dir, dataset_name)\n",
        "        self.dataframe, _ = base_dataset_loader.get_data(limit_days=limit_days,\n",
        "                                                         exclude_days=exclude_days,\n",
        "                                                         random_state=random_state,\n",
        "                                                         as_numpy=False)\n",
        "\n",
        "        # first target_size columns will be picked as the target if specified\n",
        "        self.target_size = target_size if target_size else self.dataframe.shape[1]\n",
        "\n",
        "        self.sequence_length = self.history_number\n",
        "        max_temporal_history = self.sequence_length\n",
        "\n",
        "        # we remove window+temporal history from start so we always have a full window\n",
        "        # we remove one from the end so that target/next_prices doesn't index out of bounds\n",
        "        self.indices = self.dataframe.iloc[self.past_prices_lookback_window+max_temporal_history:-2].index.tolist()\n",
        "        self.phase = phase\n",
        "\n",
        "        #### phase specific manipulation #####\n",
        "        if phase == 'train':\n",
        "            pass\n",
        "\n",
        "        elif phase == 'validation':\n",
        "            pass\n",
        "\n",
        "        elif phase == 'test':\n",
        "            # for test phase we want to append new predicted days\n",
        "            self.indices = self.dataframe.iloc[self.past_prices_lookback_window+max_temporal_history:].index.tolist()\n",
        "\n",
        "        if self.shuffle:\n",
        "            shuffle(self.indices)\n",
        "\n",
        "        print('Phase:', phase, '# of data:', len(self.indices))\n",
        "        \n",
        "        # data transforms - TODO: can compose more data transforms\n",
        "        self.past_prices_transform = transforms.Compose([\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "        self.next_prices_transform = transforms.Compose([\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "    def add_day(self, day_prices):\n",
        "        # update dataframe with new day and new index\n",
        "        new_row = dict(zip(self.dataframe.columns, day_prices))\n",
        "        current_max_index = self.dataframe.index.max()\n",
        "        # new_index = (pd.Timestamp(current_max_index) + pd.DateOffset(days=1)).strftime('%d/%m/%Y')\n",
        "        new_index = current_max_index + 1\n",
        "        new_row_series = pd.Series(new_row, name=new_index)\n",
        "\n",
        "        self.dataframe = self.dataframe.append(new_row_series)\n",
        "\n",
        "        # update indices\n",
        "        # remove oldest index and append new one (we don't want to process first day again)\n",
        "        self.indices = self.indices[1:]\n",
        "        self.indices.append(new_index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        inputs = {}\n",
        "        labels = {}\n",
        "        window_start = index - self.past_prices_lookback_window\n",
        "\n",
        "        for i in range(self.sequence_length):\n",
        "            inputs[i] = {}\n",
        "            past_prices_img = self.dataframe.iloc[window_start-i+1:index-i+1].reset_index(drop=True, inplace=False).to_numpy()\n",
        "            past_prices_img = self.past_prices_transform(past_prices_img)\n",
        "            inputs[i]['past_prices'] = past_prices_img\n",
        "\n",
        "        if self.phase in ('train', 'validation'):\n",
        "            # training/validation labels\n",
        "            next_prices = self.dataframe.iloc[index+1, :self.target_size].to_numpy().reshape(1, -1)\n",
        "            next_prices = self.next_prices_transform(next_prices)\n",
        "\n",
        "            labels['next_prices'] = next_prices\n",
        "        else:\n",
        "            # no labels for test\n",
        "            labels['next_prices'] = np.empty((1, self.target_size))\n",
        "        \n",
        "        return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TrvelUyT3Z9E"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyKIKKDL3Z9F",
        "colab": {}
      },
      "source": [
        "# Adapted from DV360 LSTM model\n",
        "\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, input_size=506, output_size=506, hidden_size=128,\n",
        "                 num_layers=3):\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        final_concat_size = 0\n",
        "\n",
        "        # CNN\n",
        "        cnn = models.resnet34(pretrained=True)\n",
        "        feats = list(cnn.children())[:-1]\n",
        "        feats[0] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.features = nn.Sequential(*feats)\n",
        "        self.intermediate = nn.Sequential(nn.Linear(\n",
        "            cnn.fc.in_features, input_size),\n",
        "            nn.ReLU())\n",
        "        final_concat_size += input_size\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=False)\n",
        "        final_concat_size += hidden_size\n",
        "\n",
        "        # Prices Regressor\n",
        "        self.predict_prices = nn.Sequential(\n",
        "            nn.Linear(final_concat_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, int(hidden_size / 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(hidden_size / 2), output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, data_input):\n",
        "        module_outputs = []\n",
        "        lstm_i = []\n",
        "        # Loop through temporal sequence of price \"images\" and pass through the cnn.\n",
        "\n",
        "        for idx, v in data_input.items():\n",
        "            x = self.features(v['past_prices'])\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.intermediate(x)\n",
        "            lstm_i.append(x)\n",
        "            # feed the current output directly into the regression network.\n",
        "            if idx == 0:\n",
        "                module_outputs.append(x)\n",
        "\n",
        "        # Feed temporal outputs of CNN into LSTM\n",
        "        i_lstm, _ = self.lstm(torch.stack(lstm_i))\n",
        "        module_outputs.append(i_lstm[-1])\n",
        "\n",
        "        # Concatenate current image CNN output and LSTM output.\n",
        "        x_cat = torch.cat(module_outputs, dim=-1)\n",
        "\n",
        "        # Feed concatenated outputs into the regession networks.\n",
        "        prediction = {'next_prices': torch.squeeze(self.predict_prices(x_cat))}\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-00zUUG3Z9H"
      },
      "source": [
        "## Train method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6b9cdM303Z9H",
        "colab": {}
      },
      "source": [
        "from torch import nn, optim\n",
        "import time\n",
        "import torch\n",
        "\n",
        "def train(model, lr, train_loader, validation_loader, epochs, device, log_interval):\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        # epoch training\n",
        "        \n",
        "        start_epoch_train = time.time()\n",
        "        epoch_prices_losses = []        \n",
        "        model.train()\n",
        "        running_prices_loss = 0.0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            # convert data and labels to device [cpu, cuda]\n",
        "            for k, v in data.items():\n",
        "                for k2, v2 in v.items():\n",
        "                    data[k][k2] = v2.float().to(device)\n",
        "            for k, v in target.items():\n",
        "                target[k] = v.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(data)\n",
        "\n",
        "            # loss, backprop, optimize\n",
        "            prices_loss = criterion(prediction['next_prices'], target['next_prices'].squeeze())\n",
        "            combined_loss = prices_loss\n",
        "            combined_loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_prices_loss += prices_loss.item()\n",
        "            if batch_idx > 0 and batch_idx % log_interval == 0:\n",
        "                # print avg batch statistics\n",
        "                avg_batch_prices_loss = running_prices_loss / log_interval\n",
        "                epoch_prices_losses.append(avg_batch_prices_loss)\n",
        "                print('[epoch: %d, batch:  %5d] prices loss: %.5f' % (epoch + 1, batch_idx + 1, avg_batch_prices_loss))\n",
        "                running_prices_loss = 0.0\n",
        "\n",
        "        if len(epoch_prices_losses) > 0 and len(epoch_prices_losses) > 0:\n",
        "            # print avg epoch statistics\n",
        "            epoch_prices_loss = sum(epoch_prices_losses) / len(epoch_prices_losses)\n",
        "            print('[avg train loss epoch %d] prices loss %.5f' % (epoch, epoch_prices_loss))\n",
        "        else:\n",
        "            print('0 epoch losses for training')\n",
        "\n",
        "        end_epoch_train = time.time()\n",
        "        epoch_elapsed = end_epoch_train - start_epoch_train\n",
        "        print('epoch %d: %f elapsed' % (epoch+1, epoch_elapsed))\n",
        "\n",
        "        if validation_loader:\n",
        "            # epoch validation\n",
        "            \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                epoch_validation_prices_losses = []\n",
        "                for batch_idx, (data, target) in enumerate(validation_loader):\n",
        "                    \n",
        "                    # convert data and labels to device [cpu, cuda]\n",
        "                    for k, v in data.items():\n",
        "                        for k2, v2 in v.items():\n",
        "                            data[k][k2] = v2.float().to(device)\n",
        "                    for k, v in target.items():\n",
        "                        target[k] = v.float().to(device)\n",
        "\n",
        "                    prediction = model(data)\n",
        "\n",
        "                    prices_loss = criterion(prediction['next_prices'], target['next_prices'].squeeze())\n",
        "\n",
        "                    epoch_validation_prices_losses.append(prices_loss.item())\n",
        "\n",
        "                if len(epoch_validation_prices_losses) > 0:\n",
        "                    epoch_validation_prices_loss = sum(epoch_validation_prices_losses) / len(epoch_validation_prices_losses)\n",
        "                    print('[avg validation loss epoch %d] prices loss %.5f' % (epoch, epoch_validation_prices_loss))\n",
        "                else:\n",
        "                    print('0 epoch losses for validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_QFv1TA3Z9K"
      },
      "source": [
        "## Test method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9jHyESJh3Z9K",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# generates a results.csv containing predictions for all existing training days + new test days for predict_days\n",
        "def test(model, predict_days, train_loader, test_loader, device):\n",
        "\n",
        "    initial_data_shape = test_loader.futureprices.dataframe.shape\n",
        "    # init output file\n",
        "    output_path = 'prediction/results.csv'\n",
        "    columns = train_loader.futureprices.dataframe.columns\n",
        "    pd.DataFrame([], columns=columns).to_csv(output_path)\n",
        "\n",
        "    output_interval = 50\n",
        "    last_output_index = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # generate predictions for existing days and save to results.csv in batches\n",
        "        output = []\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            \n",
        "            # convert data to device [cpu, cuda]\n",
        "            for k, v in data.items():\n",
        "                for k2, v2 in v.items():\n",
        "                    data[k][k2] = v2.float().to(device)\n",
        "\n",
        "            # predict and append to output\n",
        "            prediction = model(data)\n",
        "            output.append(prediction['next_prices'].detach().cpu().numpy())\n",
        "\n",
        "            if (batch_idx+1) % output_interval == 0 or (batch_idx+1) == num_batches:\n",
        "                # every output_interval iterations append predictions to results.csv file on disk\n",
        "                print('predicting training day t+1 {}/{}...'.format(\n",
        "                    batch_idx+1, num_batches))\n",
        "\n",
        "                pd.DataFrame(output, columns=columns, index=range(last_output_index, batch_idx+1))\\\n",
        "                    .to_csv(output_path,\n",
        "                            header=False,\n",
        "                            mode='a')\n",
        "                last_output_index = batch_idx + 1\n",
        "                output = []\n",
        "\n",
        "        # predict new days and save to results.csv\n",
        "        for day in range(predict_days):\n",
        "            for batch_idx, (data, _) in enumerate(test_loader):\n",
        "                \n",
        "                # convert data to device [cpu, cuda]\n",
        "                for k, v in data.items():\n",
        "                    for k2, v2 in v.items():\n",
        "                        data[k][k2] = v2.float().to(device)\n",
        "\n",
        "                # predict a new day and append it to the test dataloader\n",
        "                # so it can later be used to predict the next day\n",
        "                prediction = model(data)\n",
        "                test_loader.add_day(prediction['next_prices'].cpu().numpy())\n",
        "\n",
        "                if (day+1) % 5 == 0 or (day+1) == predict_days:\n",
        "                    print('predicting new day {}/{}...'.format(day+1, predict_days))\n",
        "\n",
        "        print('Finished | initial data shape: {} | final data shape: {}'.format(\n",
        "            initial_data_shape, test_loader.futureprices.dataframe.shape))\n",
        "\n",
        "    # append new days to results.csv file on disk\n",
        "    print('Saving results to \"{}\"...'.format(output_path))\n",
        "    output_df = test_loader.futureprices.dataframe.iloc[-predict_days:]\n",
        "    output_df.index = range(last_output_index, last_output_index+len(output_df))\n",
        "    output_df.to_csv(output_path, header=False, mode='a')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lt5LXPnX3Z9M"
      },
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B0bbN_H83Z9N",
        "colab": {}
      },
      "source": [
        "force_cpu = False\n",
        "data_dir = 'data' # directory in which the data is\n",
        "dataset_name = 'train_new_percent_change' # name of csv file in data_dir (excluding .csv extension)\n",
        "modes = ['train', 'test'] # train, test or train-and-test\n",
        "\n",
        "batch_size = 1 # training batch size - Note haven't tested with more than 1, might only work with 1 on the 'test' phase\n",
        "epochs = 5 # number of training epochs\n",
        "lr = 0.0001 # learning rate\n",
        "\n",
        "days_lookback_window = 20 # LSTM sliding window size\n",
        "history_number = 30 # number of sliding temporal windows to consider at each iteration (length of LSTM in time)\n",
        "num_layers = 1 # number of layers of LSTM model\n",
        "hidden_size = 512 # int(input_size / 4)\n",
        "\n",
        "test_predict_days = 30 # number of new days to predict\n",
        "log_interval = 50 # interval of batches to print statistics for during training\n",
        "\n",
        "limit_days = None # optional: integer limit to limit_days final days of the dataset\n",
        "val_days = None # optional: integer validation set size in days (taken from end of dataset)\n",
        "target_size = None # optional: integer number of columns to have in final prediction. If not specified will be identical to input number of columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8KqtC4kY3Z9P"
      },
      "source": [
        "## Main runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnA_pfuo3Z9Q",
        "outputId": "a673fda2-3eac-4f21-ccd3-87a0f8a2cd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "###\n",
        "# Run this to generate results.csv with target_size columns and K rows where:\n",
        "# K = min(training data, limit_days) - val_days - days_lookback_window - history_number + test_predict_days\n",
        "# The final test_predict_days rows are the new generated prediction for the new days\n",
        "###\n",
        "\n",
        "# SETUP DEVICE #\n",
        "device_type = 'cuda' if not force_cpu and torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device_type)\n",
        "print('device is: ', device)\n",
        "# END SETUP DEVICE #\n",
        "\n",
        "# SETUP DATALOADERS #\n",
        "\n",
        "val_data_dim = None\n",
        "test_data_dim = None\n",
        "validation_loader = None\n",
        "test_loader = None\n",
        "\n",
        "train_loader = FuturePricesLoader('train', batch_size, data_dir, dataset_name,\n",
        "                                      days_lookback_window,\n",
        "                                      target_size=target_size,\n",
        "                                      limit_days=limit_days,\n",
        "                                      exclude_days=val_days,\n",
        "                                      history_number=history_number)\n",
        "\n",
        "train_data_dim = train_loader.data_dim\n",
        "\n",
        "if val_days and val_days > 0:\n",
        "    validation_loader = FuturePricesLoader('validation', batch_size, data_dir, dataset_name,\n",
        "                                          days_lookback_window,\n",
        "                                          target_size=target_size,\n",
        "                                          limit_days=val_days,\n",
        "                                          history_number=history_number)\n",
        "    val_data_dim = validation_loader.data_dim\n",
        "\n",
        "\n",
        "if 'test' in modes:\n",
        "    test_loader = FuturePricesLoader('test', batch_size, data_dir, dataset_name,\n",
        "                                          days_lookback_window,\n",
        "                                          target_size=target_size,\n",
        "                                          limit_days=days_lookback_window,\n",
        "                                          history_number=history_number)\n",
        "\n",
        "    test_data_dim = test_loader.data_dim\n",
        "\n",
        "\n",
        "output_size = train_loader.futureprices.target_size if train_loader else test_loader.futureprices.target_size\n",
        "\n",
        "\n",
        "validation_dataloader = None\n",
        "# END SETUP DATALOADERS #\n",
        "\n",
        "params = {\n",
        "    'lr': lr,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'log_interval': log_interval,\n",
        "    'device': device_type,\n",
        "    'train_data_shape': train_data_dim,\n",
        "    'validation_data_shape': val_data_dim,\n",
        "    'test_data_shape': test_data_dim,\n",
        "    'test_predict_days': test_predict_days,\n",
        "    'target_size': target_size\n",
        "}\n",
        "\n",
        "print('running with params: {}'.format(params))\n",
        "\n",
        "# SETUP MODEL #\n",
        "assert (train_data_dim or test_data_dim)\n",
        "input_size = train_data_dim[1] if train_data_dim else test_data_dim[1]\n",
        "\n",
        "model = PricePredictionModel(input_size=input_size,\n",
        "                             output_size=output_size,\n",
        "                             hidden_size=hidden_size,\n",
        "                             num_layers=num_layers)\n",
        "\n",
        "if device_type == 'cuda':\n",
        "    model.cuda()\n",
        "# END SETUP MODEL #\n",
        "\n",
        "\n",
        "if 'train' in modes:\n",
        "    print('--Started training--')\n",
        "    train(model, lr, train_loader, validation_loader, epochs, device, log_interval)\n",
        "    print('--Finished training--')\n",
        "\n",
        "if 'test' in modes:\n",
        "    print('--Started testing--')\n",
        "    test(model, test_predict_days, train_loader, test_loader, device)\n",
        "    print('--Finished testing--')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device is:  cuda\n",
            "Phase: train # of data: 838\n",
            "Phase: test # of data: 1\n",
            "running with params: {'lr': 0.0001, 'batch_size': 1, 'epochs': 5, 'log_interval': 50, 'device': 'cuda', 'train_data_shape': (890, 506), 'validation_data_shape': None, 'test_data_shape': (51, 506), 'test_predict_days': 30, 'target_size': None}\n",
            "--Started training--\n",
            "[epoch: 1, batch:     51] prices loss: 0.00127\n",
            "[epoch: 1, batch:    101] prices loss: 0.00044\n",
            "[epoch: 1, batch:    151] prices loss: 0.00107\n",
            "[epoch: 1, batch:    201] prices loss: 0.00046\n",
            "[epoch: 1, batch:    251] prices loss: 0.00054\n",
            "[epoch: 1, batch:    301] prices loss: 0.00031\n",
            "[epoch: 1, batch:    351] prices loss: 0.00030\n",
            "[epoch: 1, batch:    401] prices loss: 0.00022\n",
            "[epoch: 1, batch:    451] prices loss: 0.00032\n",
            "[epoch: 1, batch:    501] prices loss: 0.00020\n",
            "[epoch: 1, batch:    551] prices loss: 0.00015\n",
            "[epoch: 1, batch:    601] prices loss: 0.00019\n",
            "[epoch: 1, batch:    651] prices loss: 0.00020\n",
            "[epoch: 1, batch:    701] prices loss: 0.00019\n",
            "[epoch: 1, batch:    751] prices loss: 0.00018\n",
            "[epoch: 1, batch:    801] prices loss: 0.00038\n",
            "[avg train loss epoch 0] prices loss 0.00040\n",
            "epoch 1: 423.002289 elapsed\n",
            "[epoch: 2, batch:     51] prices loss: 0.00059\n",
            "[epoch: 2, batch:    101] prices loss: 0.00039\n",
            "[epoch: 2, batch:    151] prices loss: 0.00105\n",
            "[epoch: 2, batch:    201] prices loss: 0.00045\n",
            "[epoch: 2, batch:    251] prices loss: 0.00053\n",
            "[epoch: 2, batch:    301] prices loss: 0.00030\n",
            "[epoch: 2, batch:    351] prices loss: 0.00030\n",
            "[epoch: 2, batch:    401] prices loss: 0.00022\n",
            "[epoch: 2, batch:    451] prices loss: 0.00031\n",
            "[epoch: 2, batch:    501] prices loss: 0.00020\n",
            "[epoch: 2, batch:    551] prices loss: 0.00014\n",
            "[epoch: 2, batch:    601] prices loss: 0.00018\n",
            "[epoch: 2, batch:    651] prices loss: 0.00020\n",
            "[epoch: 2, batch:    701] prices loss: 0.00019\n",
            "[epoch: 2, batch:    751] prices loss: 0.00018\n",
            "[epoch: 2, batch:    801] prices loss: 0.00038\n",
            "[avg train loss epoch 1] prices loss 0.00035\n",
            "epoch 2: 481.384028 elapsed\n",
            "[epoch: 3, batch:     51] prices loss: 0.00059\n",
            "[epoch: 3, batch:    101] prices loss: 0.00039\n",
            "[epoch: 3, batch:    151] prices loss: 0.00104\n",
            "[epoch: 3, batch:    201] prices loss: 0.00045\n",
            "[epoch: 3, batch:    251] prices loss: 0.00052\n",
            "[epoch: 3, batch:    301] prices loss: 0.00030\n",
            "[epoch: 3, batch:    351] prices loss: 0.00029\n",
            "[epoch: 3, batch:    401] prices loss: 0.00022\n",
            "[epoch: 3, batch:    451] prices loss: 0.00031\n",
            "[epoch: 3, batch:    501] prices loss: 0.00020\n",
            "[epoch: 3, batch:    551] prices loss: 0.00014\n",
            "[epoch: 3, batch:    601] prices loss: 0.00018\n",
            "[epoch: 3, batch:    651] prices loss: 0.00020\n",
            "[epoch: 3, batch:    701] prices loss: 0.00019\n",
            "[epoch: 3, batch:    751] prices loss: 0.00018\n",
            "[epoch: 3, batch:    801] prices loss: 0.00037\n",
            "[avg train loss epoch 2] prices loss 0.00035\n",
            "epoch 3: 423.846644 elapsed\n",
            "[epoch: 4, batch:     51] prices loss: 0.00058\n",
            "[epoch: 4, batch:    101] prices loss: 0.00038\n",
            "[epoch: 4, batch:    151] prices loss: 0.00104\n",
            "[epoch: 4, batch:    201] prices loss: 0.00045\n",
            "[epoch: 4, batch:    251] prices loss: 0.00052\n",
            "[epoch: 4, batch:    301] prices loss: 0.00030\n",
            "[epoch: 4, batch:    351] prices loss: 0.00029\n",
            "[epoch: 4, batch:    401] prices loss: 0.00022\n",
            "[epoch: 4, batch:    451] prices loss: 0.00031\n",
            "[epoch: 4, batch:    501] prices loss: 0.00020\n",
            "[epoch: 4, batch:    551] prices loss: 0.00014\n",
            "[epoch: 4, batch:    601] prices loss: 0.00018\n",
            "[epoch: 4, batch:    651] prices loss: 0.00020\n",
            "[epoch: 4, batch:    701] prices loss: 0.00019\n",
            "[epoch: 4, batch:    751] prices loss: 0.00018\n",
            "[epoch: 4, batch:    801] prices loss: 0.00037\n",
            "[avg train loss epoch 3] prices loss 0.00035\n",
            "epoch 4: 423.812854 elapsed\n",
            "[epoch: 5, batch:     51] prices loss: 0.00058\n",
            "[epoch: 5, batch:    101] prices loss: 0.00038\n",
            "[epoch: 5, batch:    151] prices loss: 0.00104\n",
            "[epoch: 5, batch:    201] prices loss: 0.00045\n",
            "[epoch: 5, batch:    251] prices loss: 0.00052\n",
            "[epoch: 5, batch:    301] prices loss: 0.00030\n",
            "[epoch: 5, batch:    351] prices loss: 0.00029\n",
            "[epoch: 5, batch:    401] prices loss: 0.00021\n",
            "[epoch: 5, batch:    451] prices loss: 0.00031\n",
            "[epoch: 5, batch:    501] prices loss: 0.00019\n",
            "[epoch: 5, batch:    551] prices loss: 0.00014\n",
            "[epoch: 5, batch:    601] prices loss: 0.00018\n",
            "[epoch: 5, batch:    651] prices loss: 0.00019\n",
            "[epoch: 5, batch:    701] prices loss: 0.00019\n",
            "[epoch: 5, batch:    751] prices loss: 0.00018\n",
            "[epoch: 5, batch:    801] prices loss: 0.00037\n",
            "[avg train loss epoch 4] prices loss 0.00035\n",
            "epoch 5: 423.689714 elapsed\n",
            "--Finished training--\n",
            "--Started testing--\n",
            "predicting training day t+1 50/838...\n",
            "predicting training day t+1 100/838...\n",
            "predicting training day t+1 150/838...\n",
            "predicting training day t+1 200/838...\n",
            "predicting training day t+1 250/838...\n",
            "predicting training day t+1 300/838...\n",
            "predicting training day t+1 350/838...\n",
            "predicting training day t+1 400/838...\n",
            "predicting training day t+1 450/838...\n",
            "predicting training day t+1 500/838...\n",
            "predicting training day t+1 550/838...\n",
            "predicting training day t+1 600/838...\n",
            "predicting training day t+1 650/838...\n",
            "predicting training day t+1 700/838...\n",
            "predicting training day t+1 750/838...\n",
            "predicting training day t+1 800/838...\n",
            "predicting training day t+1 838/838...\n",
            "predicting new day 5/30...\n",
            "predicting new day 10/30...\n",
            "predicting new day 15/30...\n",
            "predicting new day 20/30...\n",
            "predicting new day 25/30...\n",
            "predicting new day 30/30...\n",
            "Finished | initial data shape: (51, 506) | final data shape: (81, 506)\n",
            "Saving results to \"prediction/results.csv\"...\n",
            "--Finished testing--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mgqQ96aJ3Z9S"
      },
      "source": [
        "# 2. Conic weight optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "slYXdeMX3Z9S"
      },
      "source": [
        "## Helper methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9DkIrn2E3Z9T",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# conic optimization of price change covariance to weights using SLSQP method\n",
        "def optimize(num_stocks, cov, expected_val, bounds):\n",
        "#     print(num_stocks, cov.shape, expected_val.shape)\n",
        "    init_guess = np.ones(num_stocks) * (1.0 / num_stocks)\n",
        "    expected_val = expected_val.values.reshape((num_stocks, 1))\n",
        "    \n",
        "#     print('1', np.matmul(init_guess.T,np.matmul(cov,init_guess)).shape)\n",
        "#     print('2', np.matmul(init_guess,expected_val).shape, init_guess.shape, expected_val.shape)\n",
        "    \n",
        "    weights = minimize(\n",
        "        lambda w: (np.matmul(w.T,np.matmul(cov,w)) - np.matmul(w.T,expected_val)), \n",
        "        init_guess,\n",
        "        method='SLSQP', \n",
        "        options={'disp': False}, \n",
        "        constraints=({'type': 'eq', 'fun': lambda w: 1.0 - np.sum(w)}), \n",
        "        bounds=bounds\n",
        "    )\n",
        "    return weights.x\n",
        "\n",
        "\n",
        "# multiprocessing approach to optimization \n",
        "# (each row takes 30 seconds to optimize, \n",
        "#  so multithreading is crucial)\n",
        "# modifies w_ret\n",
        "def handle_optimization(i, bounds, lookback_window, prices, num_stocks, num_days):\n",
        "    # get lookback of k days from real prices and lookahead of 1\n",
        "    train = prices.iloc[i-(lookback_window-1):i+1]\n",
        "    print(train.shape)\n",
        "    print(\"optimizing row {}/{}...\".format(i-(lookback_window-1), num_days - (lookback_window-1)))\n",
        "    cov = train.cov()\n",
        "    expected_val = train.mean()\n",
        "    print('expected_val', expected_val.shape)\n",
        "    test = optimize(num_stocks, cov.values, expected_val, bounds)\n",
        "    return (i-(lookback_window-1), test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F7fgbBt83Z9V"
      },
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_JGi-zcs3Z9W",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "curr_time = datetime.now().strftime('%H%M')\n",
        "\n",
        "# input_path = f'prediction/results_{curr_time}.csv' # full path to price change csv\n",
        "# output_path = f'prediction/weights_{curr_time}.csv' # full output path for weights csv\n",
        "\n",
        "input_path = 'prediction/results.csv' # full path to price change csv\n",
        "output_path = 'prediction/weights.csv' # full output path for weights csv\n",
        "weight_min = -0.5 # minimum weights range bound\n",
        "weight_max = 2 # maximum weights range bound\n",
        "lookback_window = 5 # lookback window in days for optimization\n",
        "num_threads = 4 # number of concurrent threads to use (upper bounded by available cpus)\n",
        "target_days = test_predict_days # number of desired output rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AtrhpsM63Z9Y"
      },
      "source": [
        "## Main runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1Yz_upI3Z9Z",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import Bounds\n",
        "from functools import partial\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "\n",
        "num_threads = min(num_threads, mp.cpu_count())\n",
        "\n",
        "###\n",
        "# This conic optimizer is used to optimize a matrix of price changes \n",
        "# into a matrix of portfolio weights in a range.\n",
        "# We only want to optimize weights for the new predicted days in the \n",
        "# conic optimizer.\n",
        "# For each row the optimizer looks at today and a window of k days in the past\n",
        "###\n",
        "\n",
        "prices = pd.read_csv(input_path, index_col=0)\n",
        "\n",
        "# consider only final target_days + the lookback window\n",
        "prices = prices[-(target_days + (lookback_window - 1)):]\n",
        "\n",
        "num_days = prices.shape[0]\n",
        "num_stocks = prices.shape[1]\n",
        "\n",
        "thread_pool = mp.Pool(num_threads) # concurrency\n",
        "\n",
        "w_ret = np.zeros((target_days, prices.shape[1]))\n",
        "print('output shape: {}'.format(w_ret.shape))\n",
        "# bounds for weights. -.5/2 default\n",
        "bounds = Bounds([weight_min]*num_stocks, [weight_max]*num_stocks)\n",
        "\n",
        "# each row can take 30-60 seconds to optimize, so multithreading can be helpful\n",
        "multithread_partial = partial(\n",
        "    handle_optimization,\n",
        "    bounds=bounds,\n",
        "    lookback_window=lookback_window,\n",
        "    prices=prices,\n",
        "    num_stocks=num_stocks,\n",
        "    num_days=num_days\n",
        ")\n",
        "\n",
        "# print(range((lookback_window-1), num_days))\n",
        "optimal_weights_unsorted = thread_pool.map(\n",
        "    multithread_partial, \n",
        "    range((lookback_window-1), num_days)\n",
        ")\n",
        "\n",
        "# realign output into matrix by indices\n",
        "for i, w in sorted(optimal_weights_unsorted, key=lambda pair: pair[0]):\n",
        "    w_ret[i, :] = w\n",
        "\n",
        "print('Saving optimal weights to \"{}\"'.format(output_path))\n",
        "weights_df = pd.DataFrame(w_ret, columns=prices.columns)\n",
        "weights_df.to_csv(output_path, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "89R32-VC3Z9b",
        "colab": {},
        "outputId": "d8b71d50-4734-4ca6-bf4d-8867e15af904"
      },
      "source": [
        "weights_df = pd.read_csv('prediction/weights.csv')\n",
        "columns = [col.strip() for col in weights_df.columns]\n",
        "# print(weights_df.head())\n",
        "\n",
        "out = ['Id,Predicted']\n",
        "for ind, row in enumerate(weights_df.itertuples(index=False)):\n",
        "    for col_ind, col in enumerate(row):\n",
        "        out.append(f'{ind}_{columns[col_ind]},{col}')\n",
        "\n",
        "# write to submission file\n",
        "sub_file = 'prediction/submission.csv'\n",
        "with open(sub_file, 'w') as f:\n",
        "    for l in out:\n",
        "        f.write(l + '\\n')\n",
        "print(f'wrote to {sub_file}')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wrote to prediction/submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
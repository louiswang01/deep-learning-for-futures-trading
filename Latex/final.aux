\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{xiong2018practical}
\citation{Zhong2019}
\citation{SEZER2017473}
\citation{ganesh2018deep}
\citation{doi:10.1002/for.2585}
\citation{DBLP:conf/esann/LimG18}
\newlabel{intro}{{1}{1}{}{section.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lstm}{{1}{2}{Architecture design of LSTM model for mid price prediction. Input of the model consists of 90 time series feature vectors $x_1,…,x_{90}$, sampled with 90 seconds interval. Output of the model is fed into linear regression to produce mid price change prediction. The model is trained with stochastic gradient decent algorithm.\relax }{figure.caption.1}{}}
\newlabel{fig:rl_strat}{{2}{3}{We've defined three actions for Q-learning: buy, hold and sell.\relax }{figure.caption.2}{}}
\newlabel{fig:rl_loop}{{3}{3}{The training loop for the double DQN model for mid price prediction\relax }{figure.caption.3}{}}
\newlabel{alg:dqn}{{1}{4}{Double DQN algorithm\relax }{algorithm.1}{}}
\newlabel{fig:ddpg_flow1}{{4}{4}{We train the DDPG model using Actor-Critic method where the Critic is used to approximate Q value function for a given state-action pair and is a deep neural network, and the Actor is used to approximate optimal policy deterministically.\relax }{figure.caption.4}{}}
\newlabel{eq1}{{4.2.3}{4}{}{subsubsection.4.2.3}{}}
\newlabel{fig:ddpg_flow}{{5}{5}{General overview of DDPG algorithm flow\relax }{figure.caption.5}{}}
\newlabel{eq2}{{4.2.3}{5}{}{subsubsection.4.2.3}{}}
\newlabel{alg:ddpg}{{2}{5}{DDPG algorithm\relax }{algorithm.2}{}}
\newlabel{alg:sac}{{3}{6}{Soft Actor-Critic\relax }{algorithm.3}{}}
\newlabel{tab:mse}{{1}{6}{Performance comparison between our LSTM model and ridge regression model and OLS model\relax }{table.caption.6}{}}
\newlabel{tab:tw_in}{{2}{7}{Two-way table of in-sample training\relax }{table.caption.7}{}}
\newlabel{tab:tw_out}{{3}{7}{Two-way table of out-sample testing\relax }{table.caption.8}{}}
\newlabel{tab:rl_avg_perf}{{4}{7}{Comparison of average rewards of buy, hold and sell actions for three reinforcement algorithms: DQN, DDPG and SAC, and a market making strategy\relax }{table.caption.10}{}}
\bibstyle{icml2020}
\bibdata{final}
\bibcite{doi:10.1002/for.2585}{{1}{2019}{{Borovkova \& Tsiamas}}{{Borovkova and Tsiamas}}}
\bibcite{ganesh2018deep}{{2}{2018}{{Ganesh \& Rakheja}}{{Ganesh and Rakheja}}}
\bibcite{DBLP:conf/esann/LimG18}{{3}{2018}{{Lim \& Gorse}}{{Lim and Gorse}}}
\newlabel{fig:perf_compare}{{6}{8}{Comparison of in-sample learning performance (measured by moving average of returns over most recent 10 epochs of training) for the three deep RL models. From top to bottom are for DQN, DDPG and SAC respectively. Out-sample backtesting period is 10/01/2018 – 12/31/2018\relax }{figure.caption.9}{}}
\newlabel{fig:bt_pnl}{{7}{8}{Comparison of cumulative trading PnL for each of the 7 trading strategies (strategies based on OLS, Ridge, LSTM, DQN, DDPG, SAC, and the benchmark market making strategy) over the out-sample backtesting period (10/01/2018 – 12/31/2018)\relax }{figure.caption.11}{}}
\newlabel{tab:perf_stats}{{5}{8}{Trading performance statistics for for each of the 7 trading strategies (strategies based on OLS, Ridge, LSTM, DQN, DDPG, SAC, and the benchmark market making strategy). In-sample data is shown in the upper table while out-sample data is shown in the lower table. In-sample period is 01/01/2018 – 8/31/2018, and out-sample period is 10/01/2018 – 12/31/2018\relax }{table.caption.12}{}}
\bibcite{SEZER2017473}{{4}{2017}{{Sezer et~al.}}{{Sezer, Ozbayoglu, and Dogdu}}}
\bibcite{xiong2018practical}{{5}{2018}{{Xiong et~al.}}{{Xiong, Liu, Zhong, Yang, and Walid}}}
\bibcite{Zhong2019}{{6}{2019}{{Zhong \& Enke}}{{Zhong and Enke}}}

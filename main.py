# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dog0GIPc-QFFzeO1TWRVbbQGootsq1Sl
"""

""" mount drive """

import os

GOOGLE_DIR = '/content/drive/My Drive/DL_Project'
LOCAL_DIR = '/home/ziyan/Desktop/Deep Learning Project'

def colab_mount_google_drive():
    drive.mount('/content/drive', force_remount=True)
    os.chdir(GOOGLE_DIR)
    os.listdir()

def mount_local_drive():
    os.chdir(LOCAL_DIR)
    os.listdir()

try:
    from google.colab import drive
    colab_mount_google_drive()
    DIR = GOOGLE_DIR
    print('Mounted google drive')
except ModuleNotFoundError:
    mount_local_drive
    DIR = LOCAL_DIR
    print('Mounted local drive')

""" model specification """

# for dataset
product = 'input_df_cross_assets_v2'

fields = [
    'mid_lag_10s', 'mid_lag_30s', 'mid_lag_01m',
    'wmid_ma_05m', 'wmid_ma_10m', 
    'wmid_max_05m', 'wmid_max_10m', 
    'wmid_min_05m', 'wmid_min_10m', 
    'wmid_bidask_01m',
    'total_volume_10s', 'total_volume_01m', 
    'signed_volume_10s', 'signed_volume_01m',
    'signed_tick_10s', 'signed_tick_01m',
    'IF_mid_lag_05s', 'IF_mid_lag_30s',
    'IF_total_volume_10s', 'IF_total_volume_01m',
    'IC_mid_lag_05s', 'IC_mid_lag_30s',
    'IC_total_volume_10s', 'IC_total_volume_01m'
]  # column names
y_field = 'mid_30s'

series_length = 60  # number of samples
sample_interval = 30  # sample every 30 seconds
cache_limit = 300

use_cuda = True

# for train and validate dataloader
params = {
    'batch_size': 1,
    'shuffle': True,
    'num_workers': 3
}

train_sd = '20180102'
train_ed = '20180930'
validate_sd = '20181001'
validate_ed = '20181031'
test_sd = '20181101'
test_ed = '20181231'

import pandas as pd
df1 = pd.read_csv('input_df_cross_assets_v2/input_20181101.csv.gz')
# df1[fields].iloc[0]
#df1['mid_lag_30s'].iloc[3565:3580]
# df1.head()
df1.columns

""" create index table for selected product """
import re

os.chdir(os.path.join(DIR, product))
table_file = f'{product}_train_indexes_{series_length}_{sample_interval}.csv'

if not os.path.isfile(table_file):  # skip this step if the index tables already exists (only check train here)
    files = {'train': [], 'validate': [], 'test': []}  # dict of list of dicts
    
    for fn in sorted(os.listdir()):
        # rows number is first subtracted by {series_length} at the begining
        m = re.search('input_(2018\d{4})', fn)
        if m:
            date = m.group(1)
            if date <= train_ed:
                data_type = 'train'
            elif date <= validate_ed:
                data_type = 'validate'
            else:
                data_type = 'test'
            
            df = pd.read_csv(fn)
            cutoff = series_length * sample_interval / 0.5  # drop the tailing indexes
            files[data_type].append({'file_name': fn, 'rows': int(df.shape[0] - cutoff - 1)})

    for data_type in files:
        table = pd.DataFrame(files[data_type])
        table['date'] = table['file_name'].str.split('_').str[-1].str.split('.').str[0]
        table_file = f'{product}_{data_type}_indexes_{series_length}_{sample_interval}.csv'
        table.to_csv(f'{table_file}', index=False)
        print(f'Wrote {data_type} index table to {product}/{table_file}')
else:
    print('Skipped.')

os.chdir(os.path.join(DIR))

""" Create data generators """

from torch.utils import data
from dataset import Dataset

# create a train, validation and test data loader
train_set = Dataset(train_sd, train_ed, product=product, data_type='train', x_fields=fields, y_field=y_field, 
                    series_length=series_length, sample_interval=sample_interval,
                    cache_limit=cache_limit, last_output_only=False)
train_loader = data.DataLoader(train_set, **params)

validation_set = Dataset(validate_sd, validate_ed, product=product, data_type='validate', x_fields=fields, y_field=y_field,
                         series_length=series_length, sample_interval=sample_interval,
                         cache_limit=cache_limit, last_output_only=False)
validation_loader = data.DataLoader(validation_set, **params)

test_set = Dataset(test_sd, test_ed, product=product, data_type='test', x_fields=fields, y_field=y_field,
                   series_length=series_length, sample_interval=sample_interval,
                   cache_limit=cache_limit, last_output_only=False)
# for test only
test_params = {
    'batch_size': 1,
    'shuffle': False,
    'num_workers': 3
}
test_loader = data.DataLoader(test_set, **test_params)

""" 
Init and train model

Main issues:
1. tensor shape for LSTM (3 dimensions with batch as the middle one)
2. model (parameter) and data precision should match by using float()
"""

import gc
import torch.optim as optim
from time import time
from models import *

if use_cuda:
    model = PrototypeModel(input_size=len(fields), num_layers=series_length).float().cuda()  # use float precision instead of double
else:
    model = PrototypeModel(input_size=len(fields), num_layers=series_length).float()

criterion = nn.SmoothL1Loss()

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.6)
model.train()
epoch_error = {}
out = []

for epoch in range(1):
    running_loss = 0.0
    start_time = time()
    epoch_error[epoch] = []
    batch_idx = 0
    
    for data, target in train_loader:
        batch_idx += 1
        optimizer.zero_grad()
#         print(data)
        if use_cuda:
            prediction = model(data.float().cuda())
        else:
            prediction = model(data.float())
        
        if use_cuda:
            loss = criterion(prediction, target.cuda())
        else:
            loss = criterion(prediction, target)
        
        loss.backward()
        optimizer.step()
        
        # print statistics
        running_loss += loss.item()
        cost_time = time() - start_time
#         print(prediction, target)
        print('[epoch: %d, batch:  %5d] target: %6.2f | pred: %6.2f | loss: %.5f  | %.2f' %
              (epoch + 1, batch_idx + 1, target, prediction, running_loss, cost_time))
        
        out.append([float(target), float(prediction)])
        running_loss = 0.0
        start_time = time()
        
        epoch_error[epoch].append(loss)
        
        # Remove this when actually training. 
        # Used to terminate early. 
        if batch_idx > 50000: 
            break
            
#     gc.collect()  # does it help?

"""Local Evaluation / Test"""

import numpy as np

test_mode = True

model.eval()
out = []
# model.cuda()
loader = validation_loader if not test_mode else test_loader

with torch.no_grad():
    mse = []
    batch_idx = 0
    
    for data, target in loader:
        batch_idx += 1
#         print(data, target)
        prediction = model(data.float().cuda())
        
        mse.append(np.square(prediction.cpu() - target))
        if test_mode:
            out.append([float(target), float(prediction)])
        
        if test_mode:
            print('[epoch: %d, batch:  %5d] target: %6.2f | pred: %6.2f' %
                  (epoch + 1, batch_idx + 1, target, prediction))
        # Used to terminate early, remove.
        if batch_idx >= 1000: 
            break

print('MSE: %.2f' % (np.array(mse).mean()))
if test_mode:
    print(out)

for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param.data)

with open('test result_2.csv', 'w') as f:
    for lst in out:
        f.write(','.join([str(i) for i in lst])+'\n')

import torch.optim as optim
from time import time
from models import *


model = LinearRegressionModel(len(fields)).cuda()
criterion = nn.SmoothL1Loss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.6)

batch_idx=0
out=[]
for data, target in train_loader:
        batch_idx += 1
        optimizer.zero_grad()
#         print(data)
        if use_cuda:
            prediction = model(data.float().cuda())
        else:
            prediction = model(data.float())
        
        if use_cuda:
            loss = criterion(prediction, target.float().cuda())
        else:
            loss = criterion(prediction, target.float())
        
        loss.backward()
        optimizer.step()
        # print('target', target)
        # print('pred', prediction)
        # print('loss', loss)

        print('[epoch: %d, batch:  %5d] target: %6.2f | pred: %6.2f | loss: %.5f ' %
              (1, batch_idx + 1, target.mean(), prediction.mean(), loss.mean()))
        
        for i in range(len(target)):
          out.append([float(target[i][0]), float(prediction[i])])
        
        # Remove this when actually training. 
        # Used to terminate early. 
        if batch_idx > 10000: 
            break

print(len(train_set.x_fields), train_set.sample_interval)

"""Local Evaluation / Test"""

import numpy as np

test_mode = True

model.eval()
out = []
# model.cuda()
loader = validation_loader if not test_mode else test_loader

with torch.no_grad():
    mse = []
    batch_idx = 0
    
    for data, target in loader:
        batch_idx += 1
        
        prediction = model(data.float().cuda())
        
        for i in range(len(prediction)):
            if test_mode:
                print('[batch:  %5d] target: %6.2f | pred: %6.2f' %
                      (batch_idx, target[0][i][0], prediction[i]))
                out.append([float(target[0][i][0]), float(prediction[i])])
        
            mse.append(np.square(prediction[i].cpu() - target[0][i][0]))
            
        
        # Used to terminate early, remove.
        if batch_idx >= 1000: 
            break

print('MSE: %.2f' % (np.array(mse).mean()))
if test_mode:
    print(out)

np.array(mse).mean()

import pandas as pd
df=pd.DataFrame(out, columns=['target', 'pred'])

df.head()

def group_values(x):
    if x >= 0.04:
        return '>0.04'
    if 0.02 > x >= 0:
        return '>0'
    elif 0.04 > x >= 0.02:
        return '<0.02'
    elif x >= -0.02:
        return '>-0.02'
    elif x >= -0.04:
        return '>-0.04'
    else:
        return '<-0.04'

df['target']=df['target'].apply(group_values)
df['pred']=df['pred'].apply(group_values)
print(df.head())

pd.crosstab([df['target']],
            [df['pred']])

len(out)